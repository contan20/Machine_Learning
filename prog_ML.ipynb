{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "63ed59a4-682f-4fb3-98ce-92c140ed8b3a",
    "_uuid": "3d69d910-0994-4a97-98d3-5b881b837b49",
    "id": "Cc2z-9hb6ORO"
   },
   "source": [
    "<a id='beginning'></a>\n",
    "# Machine Learning Challenge\n",
    "#### Student: Andrea Contenta\n",
    "#### Professor: Thibaud Vienne\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "747830bc-d36d-4a46-941e-a988367862a0",
    "_uuid": "b8e55a5e-e927-4d26-ba78-a5cae5bd65f4"
   },
   "source": [
    "The workbook is organized as follows:\n",
    "\n",
    "\n",
    "0. **Prerequisites**:\n",
    "    >- about data\n",
    "    >- load modules\n",
    "    >- functions\n",
    "\n",
    "\n",
    "1. **Data exploration**:\n",
    "    >- load data and set index\n",
    "    >- look at the distribution of the target\n",
    "    >- data visualization\n",
    "    >- correlation matrix\n",
    "    \n",
    "    \n",
    "2. **Features engineering and selection**:\n",
    "    >- compute new features\n",
    "    >- PCA\n",
    "    >- features selection\n",
    "    \n",
    "3. **Machine Learning algorithms experiments**:\n",
    "    >- validation procedure: random forest, xgboost, LASSO, neural network\n",
    "    >- tuned candidate models\n",
    "   \n",
    "4. **Final run**:\n",
    "    >- run best model and make predictions on validation set\n",
    "    >- interpretation\n",
    "    >- conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d400c0ac-e51c-4522-b9fd-661fb2c348db",
    "_uuid": "bad25fb6-b80a-46aa-9b50-79861713a90b",
    "id": "ZObgFUkr6ORX"
   },
   "source": [
    "# Step 0 - Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c8cca446-55fa-428e-bae5-00bed18dcd0d",
    "_uuid": "f21e69fe-fe55-4058-b391-aea5e56c3000",
    "id": "58exg1_b6ORa"
   },
   "source": [
    "<a id='0.1'></a>\n",
    "## 0.1) About data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4e6cb44e-b117-4023-b6ea-3427c5578039",
    "_uuid": "c21e45fb-0f16-44e2-a381-710e01d9d352",
    "id": "p7DPr3Mh6ORb"
   },
   "source": [
    "Dimension : Less than 1,000,000 rows, including 200,000 bid-ask spreads to predict.\n",
    "\n",
    "\n",
    "> - **product_id** and **liquidity_rank** define a unique futures contract. The dataset contains around 100 product_id (corresponding for instance to a specific type of crude oil). For each product_id several liquidity ranks can exist: liquidity_rank 0 refers to the contract with the closest expiry date (which is often the most traded one), 1 to the next contract to expire, etc. Thus, a contract of rank 1 becomes a contract of rank 0 when the previous contract of rank 0 expires.\n",
    "> - **dt_close** represents the day number (they are therefore chronological) for each data sample. For a given dt_close there is 1 entry per (product_id, liquidity_rank) pair.\n",
    "> - **dt_expiry** similarly represents the date of the futures contract expiry.\n",
    "> - **normal_trading_day** is set to 0 when the market is closed or the market activity is reduced.\n",
    "> - **open**, **close** represent contract prices, resp. at market opening and close.\n",
    "> - **high**, **low** represent the highest and lowest price of the contract during the day.\n",
    "> - **volume** is the number of contracts exchanged during the day, up to a factor that depends only on product_id.\n",
    "> - **open_interest** is the number of active contracts at the end of the day, with the same factor applied as for the volume.\n",
    "> **spread** is the “daily bid-ask spread”, which is the value we want to predict.\n",
    "> **tick_size** is a proxy for 1 unit of spread (this is not directly price difference).\n",
    "> - **fixed** is set to 1 when one or more features have been fixed for various reason (outliers, missing values…).\n",
    "\n",
    "Prices are all normalized in some way (the values are thus not in currency units), but they are consistent within each product_id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6db3e10e-0a88-4c28-8de8-6d08f4d59ca0",
    "_uuid": "a187e588-e613-4b62-a69e-458c7adbbdff",
    "id": "s_1-n2256ORc"
   },
   "source": [
    "<a id='0.2'></a>\n",
    "## 0.2) Load modules\n",
    "\n",
    "In this cell, you can put all modules you use. You can use it to provide a clear code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "0fdef92a-ff96-4c65-91dd-fcc7a0bad7c2",
    "_uuid": "ce688ec9-aff0-4574-85e6-5849bf84a229",
    "execution": {
     "iopub.execute_input": "2022-04-09T08:19:17.836330Z",
     "iopub.status.busy": "2022-04-09T08:19:17.835866Z",
     "iopub.status.idle": "2022-04-09T08:19:17.844184Z",
     "shell.execute_reply": "2022-04-09T08:19:17.843479Z",
     "shell.execute_reply.started": "2022-04-09T08:19:17.836294Z"
    },
    "id": "xyx4QRZW6ORd",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.multivariate.pca import PCA as PCA\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "abffa9c3-65b4-4755-ae24-2f1f0e400d5d",
    "_uuid": "da4208e0-06cb-440b-80e8-62a1e35c0416",
    "id": "jc27HApJ6ORe"
   },
   "source": [
    "<a id='0.3'></a>\n",
    "## 0.3) Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "5d9cd7ec-6dd9-4a8b-92c2-c20eec71a7ef",
    "_uuid": "ddeff7b4-2a0c-4d1c-b758-7286e7e0dbb3",
    "execution": {
     "iopub.execute_input": "2022-04-09T08:19:19.368272Z",
     "iopub.status.busy": "2022-04-09T08:19:19.367720Z",
     "iopub.status.idle": "2022-04-09T08:19:19.391887Z",
     "shell.execute_reply": "2022-04-09T08:19:19.391074Z",
     "shell.execute_reply.started": "2022-04-09T08:19:19.368236Z"
    },
    "id": "NZj6UhJl6ORf",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def export_ens(df_test, pred_test, save=True, path_save=\"y_test_prediction.csv\"):\n",
    "    \"\"\"\n",
    "    Export submissions with the good ENS data challenge format.\n",
    "    df_test : (pandas dataframe) test set\n",
    "    proba_test : (numpy ndarray) prediction as a numpy ndarray you get using method .predict()\n",
    "    save : (bool) if set to True, it will save csv submission in path_save path.\n",
    "    path_save : (str) path where to save submission.\n",
    "    return : dataframe for submission\n",
    "    \"\"\"\n",
    "    df_submit = pd.Series(pred_test[:], index=df_test.index, name=\"spread\")\n",
    "    df_submit.to_csv(path_save, index=True)\n",
    "    return df_submit\n",
    "\n",
    "def check_test(result, expected, display_error):\n",
    "    \"\"\"\n",
    "    Testing your results.\n",
    "    \"\"\"\n",
    "    if result == expected:\n",
    "        print(\"1 test passed.\")\n",
    "    else:\n",
    "        print(display_error)\n",
    "\n",
    "# Reduce memory usage up to 60%\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def filter_data(df, column, position):\n",
    "    \"\"\"\n",
    "    Data visualization tool\n",
    "    \"\"\"\n",
    "    unique_keys = df[column].unique()\n",
    "    return df[df[column]==unique_keys[position]]\n",
    "\n",
    "def data_pipeline(df, pipe):\n",
    "    \"\"\"\n",
    "    Apply different functions to the dataframe.\n",
    "    df : (pandas dataframe) data to transform\n",
    "    pip : (iterable of functions) functions to be applied to the data\n",
    "    return : df with data transformed\n",
    "    \"\"\"\n",
    "    error_log = []\n",
    "    for func in tqdm(pipe):\n",
    "        try:\n",
    "            df = func(df)\n",
    "        except Exception as e:\n",
    "            error_log.append(f\"An error occured while trying to apply {func.__name__} with error : {e}\")\n",
    "    print(*error_log, sep=\"\\n\")\n",
    "    return df\n",
    "\n",
    "def evaluate(df1, df2):  # TO CHECK #\n",
    "    \"\"\"\n",
    "    Compute and prints RMSE for train and validation dataset of a given model\n",
    "    \"\"\"\n",
    "    print(\"RMSE score on train dataset : %.4f\" % mean_squared_error(df1, y_train, squared=False))\n",
    "    print(\"RMSE score on validation dataset : %.4f\" % mean_squared_error(df2, y_val, squared=False))\n",
    "    \n",
    "def rescue_me():\n",
    "    \"\"\"\n",
    "    Re-load the dataset and perform manipulations on it\n",
    "    \"\"\"\n",
    "    global df_train, y_train, df_test, df_full\n",
    "  \n",
    "  # overwrite dataframes\n",
    "    df_train = pd.read_csv(\"C:/Users/andre/Downloads/input_training_imet9ZU.csv\")\n",
    "    y_train = pd.read_csv(\"C:/Users/andre/Downloads/output_training_yCN1f2d.csv\")\n",
    "    df_test = pd.read_csv(\"C:/Users/andre/Downloads/input_test_4AhEauI.csv\")\n",
    "    df_full = pd.concat([df_train,y_train], axis=1)\n",
    "    df_train = df_train.set_index('ID')\n",
    "    y_train = y_train.set_index('ID')\n",
    "    df_test = df_test.set_index('ID')\n",
    "    \n",
    "    def maturity(df):\n",
    "        return (df['dt_expiry'] - df['dt_close'])\n",
    "\n",
    "    df_train['mat']=maturity(df_train)\n",
    "    df_test['mat']=maturity(df_test)\n",
    "\n",
    "    # compute mean_target_per_product\n",
    "    mean_target_per_product = df_full.groupby([\"product_id\",\"liquidity_rank\"])[\"spread\"].mean()\n",
    "    mean_target_per_product.name = \"mean_target_per_product\"\n",
    "    df_train = df_train.merge(mean_target_per_product, how=\"left\", right_index=True, left_on=[\"product_id\", \"liquidity_rank\"])\n",
    "    df_test = df_test.merge(mean_target_per_product, how=\"left\", right_index=True, left_on=[\"product_id\", \"liquidity_rank\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "56c3bff1-ae06-4cac-bd58-3a96758647ea",
    "_uuid": "bc0416fc-e22f-424c-8d51-0855b0c9a82e",
    "execution": {
     "iopub.execute_input": "2022-04-06T12:04:34.483849Z",
     "iopub.status.busy": "2022-04-06T12:04:34.483493Z",
     "iopub.status.idle": "2022-04-06T12:04:34.58604Z",
     "shell.execute_reply": "2022-04-06T12:04:34.585264Z",
     "shell.execute_reply.started": "2022-04-06T12:04:34.483817Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rescue_me()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0983ce31-7de7-4bc5-8cab-bc2cafcd0630",
    "_uuid": "012f4ed8-5f8b-4cc0-89fb-862fc685ce7b",
    "id": "7C6SDNZg6ORg"
   },
   "source": [
    "<a id='1'></a>\n",
    "# Step 1 - Data Exploration\n",
    "Data exploration is a common part in a machine learning pipeline. In this section, you will import datasets, discover  features, provide data mining observations, investigate missing values and possible outliers. An exhaustive exploration is more likely to yield prowerful predictive models.\n",
    "\n",
    "- Read the train dataset inputs and assign to the variable **df_train**.\n",
    "- Read the train dataset labels and assign to the variable **y_train**.\n",
    "- Read the test dataset inputs and assign to the variable **df_test**.\n",
    "- Concat **df_train** and **y_train** to create **df_full**.\n",
    "- Assign number of rows in train dataset and assign it to variable **n_rows_train**.\n",
    "- Assign number of rows in test dataset and assign it to variable **n_rows_test**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8a02097d-26a3-4540-80e7-34ca08721e70",
    "_uuid": "88325861-0bcd-4c05-a639-260a662fc89a",
    "id": "TuNuZCCR6ORh"
   },
   "source": [
    "<a id='1.1'></a>\n",
    "## 1.1) Load datasets, set ID as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "ee134e94-8d52-46cf-9171-73b63b6d0924",
    "_uuid": "abb2f4cb-2a25-4ef9-bac3-871c2b85a5d7",
    "execution": {
     "iopub.execute_input": "2022-04-09T08:19:21.742306Z",
     "iopub.status.busy": "2022-04-09T08:19:21.742054Z",
     "iopub.status.idle": "2022-04-09T08:19:24.029146Z",
     "shell.execute_reply": "2022-04-09T08:19:24.028331Z",
     "shell.execute_reply.started": "2022-04-09T08:19:21.742277Z"
    },
    "id": "Lpkufn0V6ORi",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# when changing the name of the directory, recall to do it also in the function section regarding the \"rescue_me\" function (the last one)\n",
    "df_train = pd.read_csv(\"C:/Users/andre/Downloads/input_training_imet9ZU.csv\")\n",
    "y_train = pd.read_csv(\"C:/Users/andre/Downloads/output_training_yCN1f2d.csv\")\n",
    "df_test = pd.read_csv(\"C:/Users/andre/Downloads/input_test_4AhEauI.csv\")\n",
    "df_full = pd.concat([df_train,y_train], axis=1)\n",
    "n_rows_train = len(df_train)\n",
    "n_rows_test = len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "eed1f8d7-08ed-4433-b889-1dc201922f2b",
    "_uuid": "63a945b2-5ad8-43df-b70e-e3ed5373ce19",
    "execution": {
     "iopub.execute_input": "2022-04-09T08:19:24.031775Z",
     "iopub.status.busy": "2022-04-09T08:19:24.031248Z",
     "iopub.status.idle": "2022-04-09T08:19:24.040436Z",
     "shell.execute_reply": "2022-04-09T08:19:24.039719Z",
     "shell.execute_reply.started": "2022-04-09T08:19:24.031716Z"
    },
    "id": "buV7uK0a6ORj",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "c153f175-ce84-4e90-8476-fe5442534a01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "------------------------\n",
      "df_train : (629611, 14)\n",
      "y_train  : (629611, 2)\n",
      "df_test  : (230304, 14)\n",
      "df_full  : (629611, 16)\n"
     ]
    }
   ],
   "source": [
    "print('Shapes')\n",
    "print('------------------------')\n",
    "print(\"df_train : %s\" % str(df_train.shape))\n",
    "print(\"y_train  : %s\" % str(y_train.shape))\n",
    "print(\"df_test  : %s\" % str(df_test.shape))\n",
    "print(\"df_full  : %s\" % str(df_full.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "cde49166-dbf7-40ca-a4c6-7412d5c4a576",
    "_uuid": "7986bbd3-1ad3-42bd-a5a3-0e696f9ffe3b",
    "execution": {
     "iopub.execute_input": "2022-04-09T08:19:25.499787Z",
     "iopub.status.busy": "2022-04-09T08:19:25.499513Z",
     "iopub.status.idle": "2022-04-09T08:19:25.505238Z",
     "shell.execute_reply": "2022-04-09T08:19:25.504306Z",
     "shell.execute_reply.started": "2022-04-09T08:19:25.499753Z"
    },
    "id": "JxczqHUx6ORk",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "abacb8b0-9e9b-4971-b749-e538273c17cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "check_test(n_rows_train, 629611, \"wrong number of rows\")\n",
    "check_test(n_rows_test, 230304, \"wrong number of rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "a32de0d4-2dc2-4b79-ba77-54b8f75d2fe7",
    "_uuid": "d4e0fd02-216e-4621-a1ae-bdd2c75f5cf7",
    "execution": {
     "iopub.execute_input": "2022-04-09T08:19:26.211646Z",
     "iopub.status.busy": "2022-04-09T08:19:26.211040Z",
     "iopub.status.idle": "2022-04-09T08:19:26.262445Z",
     "shell.execute_reply": "2022-04-09T08:19:26.261696Z",
     "shell.execute_reply.started": "2022-04-09T08:19:26.211608Z"
    },
    "id": "bDgoHaul6ORo",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_train = df_train.set_index('ID')\n",
    "y_train = y_train.set_index('ID')\n",
    "df_test = df_test.set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "9c3c4e4f-9c9b-480b-8f70-5a0e6b8524fc",
    "_uuid": "8d1f3dc9-e9bc-4a6b-89fd-d375f3f19c0f",
    "execution": {
     "iopub.execute_input": "2022-04-09T08:19:28.531109Z",
     "iopub.status.busy": "2022-04-09T08:19:28.530602Z",
     "iopub.status.idle": "2022-04-09T08:19:28.539362Z",
     "shell.execute_reply": "2022-04-09T08:19:28.538608Z",
     "shell.execute_reply.started": "2022-04-09T08:19:28.531068Z"
    },
    "id": "Onc8vAJB6ORp",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "2677302d-4344-4672-aeaa-9d66e431783c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "check_test(df_train.shape[1], 13, \"wrong number of columns\")\n",
    "check_test(y_train.shape, (629611, 1), \"wrong number of columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "04861c23-bd6e-4f8f-b123-3a332d01aae3",
    "_uuid": "d5ca5ff8-541d-4287-be39-2c1c585e1bde"
   },
   "source": [
    "Look at the type of data included in the train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "abeaeabd-b154-4411-84bd-7a243f8c8182",
    "_uuid": "0a8515a6-2420-4494-bd7c-d3fba2307d3f",
    "execution": {
     "iopub.execute_input": "2022-04-07T07:13:00.335688Z",
     "iopub.status.busy": "2022-04-07T07:13:00.33499Z",
     "iopub.status.idle": "2022-04-07T07:13:00.388469Z",
     "shell.execute_reply": "2022-04-07T07:13:00.387616Z",
     "shell.execute_reply.started": "2022-04-07T07:13:00.335635Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d25bcc42-c4ae-4afb-b539-0caaeafab1d8",
    "_uuid": "b8ec21a6-6a7b-49b7-bedd-f1707cfff214",
    "id": "2_m4TngD6ORq"
   },
   "source": [
    "<a id='1.2'></a>\n",
    "## 1.2) Look at the distribution of the target\n",
    "\n",
    "Let's take a look on the column label (spread) in the train dataset. Take a look on the disribution of the label by plotting an histogram with matplotlib.\n",
    "\n",
    "- Plot the histogram of the serie **y_train**. You can use the parameter **bins** to change the number of slots.\n",
    "- Describe the serie **y_train** using the method **.describe()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2c877d23-715c-4b53-bbed-15964de3bfb2",
    "_uuid": "12579d00-e7c6-48ce-b152-cbc0be3ef1b7",
    "execution": {
     "iopub.execute_input": "2022-04-07T07:13:05.168596Z",
     "iopub.status.busy": "2022-04-07T07:13:05.167808Z",
     "iopub.status.idle": "2022-04-07T07:13:05.558758Z",
     "shell.execute_reply": "2022-04-07T07:13:05.557855Z",
     "shell.execute_reply.started": "2022-04-07T07:13:05.168545Z"
    },
    "id": "Pzx3KV1d6ORq",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "45be5155-2277-4ebb-979b-3e9132afd204"
   },
   "outputs": [],
   "source": [
    "# Plot the histogram\n",
    "# to make the code faster you can select a random subsample of the population\n",
    "plt.xlabel('spread')\n",
    "plt.ylabel('frequency')\n",
    "plt.suptitle('Distribution of spread')\n",
    "plt.hist(y_train, bins=50)\n",
    "plt.show()\n",
    "\n",
    "# Describe the serie of label with the method .describe()\n",
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b276facc-9d30-4558-a1e9-3fc6b69e9707",
    "_uuid": "e3588482-d57c-4de8-bf31-67f19a8c8e0f",
    "id": "FbGOYYr06ORr"
   },
   "source": [
    "Mind that over all variables, many are just categorical variables expressed in numbers. Those variables don't need to be standardized. Rather, they should be one-hot-encoded:\n",
    "\n",
    "- **categorical variables**: dt_close, product_id, liquidity_rank, normal_trading_day, dt_expiry (5)\n",
    "- **numerical variables**: open, high, low, close, open_interest, volume, tick_size (7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c97f1c8b-cc13-4ff6-b832-b108c4a75b5e",
    "_uuid": "8de39c64-86f5-459e-b737-47668db3ead6",
    "id": "BGMvxNHf6ORs"
   },
   "source": [
    "### Bid-ask spread with respect to the liquidity rank\n",
    "\n",
    "Very naively, one might think that there is a relationship between the bid-ask spread and the liquidity rank. To verify this hypothesis, we can visualize the boxplot distribution per category rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "46d6c9c4-7f04-4b94-be9f-23135b268492",
    "_uuid": "47a034f1-ded0-43f8-bbba-beeef2953381",
    "execution": {
     "iopub.execute_input": "2022-04-07T07:13:58.029488Z",
     "iopub.status.busy": "2022-04-07T07:13:58.02874Z",
     "iopub.status.idle": "2022-04-07T07:13:58.600733Z",
     "shell.execute_reply": "2022-04-07T07:13:58.59994Z",
     "shell.execute_reply.started": "2022-04-07T07:13:58.02945Z"
    },
    "id": "CeYNZ4r76ORt",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "f891ea67-9de4-4ef3-e4b4-8c656d7bcab7"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 8])\n",
    "ax = sns.boxplot(x='liquidity_rank', y='spread', data=df_full)\n",
    "plt.title(\"Distribution of the target per liquidity rank\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "709368d8-167a-4e71-a285-ddd3b26337f7",
    "_uuid": "b7d07677-5ecd-4257-bb0b-6c94ac6aa52e"
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 1.3) Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "19ec0d95-1e79-4c76-8352-14c56695aa7f",
    "_uuid": "b7b39f42-8efd-4541-9480-6a7fc6441885",
    "execution": {
     "iopub.execute_input": "2022-04-07T07:14:02.151774Z",
     "iopub.status.busy": "2022-04-07T07:14:02.151458Z",
     "iopub.status.idle": "2022-04-07T07:14:05.219598Z",
     "shell.execute_reply": "2022-04-07T07:14:05.218699Z",
     "shell.execute_reply.started": "2022-04-07T07:14:02.1517Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.subplot(1,2,1)\n",
    "sns.distplot(df_full['spread']).set(title='Distribution of spread')\n",
    "plt.subplot(1,2,2)\n",
    "sns.boxplot(df_full['spread']).set(title='Box-plot of spread')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "65540f2c-4352-4cd9-9901-d91cb6e7b54f",
    "_uuid": "97d98311-acbf-4e4a-84c1-267780736140",
    "execution": {
     "iopub.execute_input": "2022-04-07T07:25:59.266457Z",
     "iopub.status.busy": "2022-04-07T07:25:59.266198Z",
     "iopub.status.idle": "2022-04-07T07:26:02.846816Z",
     "shell.execute_reply": "2022-04-07T07:26:02.846134Z",
     "shell.execute_reply.started": "2022-04-07T07:25:59.266427Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 6, figsize=(20, 8), facecolor='w', edgecolor='k',sharex=True, sharey=True)\n",
    "\n",
    "axs = axs.ravel()\n",
    "fig.suptitle('Spread and close price for each liquidity rank')\n",
    "for i,ax in zip(range(0,12),axs):\n",
    "    ax.scatter(df_full[df_full.liquidity_rank==i].close,df_full[df_full.liquidity_rank==i].spread)\n",
    "    ax.set_xlabel('close')\n",
    "    ax.set_ylabel('spread')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8bae35c8-07be-4625-97c7-ce6af9035060",
    "_uuid": "5707d9d7-7f7f-4380-acf9-ed38e974917e"
   },
   "source": [
    "We see that the distribution of the data is very asymmetric with regard to the liquidity rank. So we want to investigate whether this asymmetry is also present in the validation dataset. If this is not the case, we would need to treat the data witha high liquidity rank (ie, less liquid) as outliers. If this is not the case, we need to figure out a way to model well those kind of extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "aa0d8d7c-aa92-49e9-84c8-32d5b83babe0",
    "_uuid": "451ada3d-c260-4897-bdb0-85d37bfec253",
    "execution": {
     "iopub.execute_input": "2022-04-07T08:16:13.084192Z",
     "iopub.status.busy": "2022-04-07T08:16:13.083627Z",
     "iopub.status.idle": "2022-04-07T08:16:13.501348Z",
     "shell.execute_reply": "2022-04-07T08:16:13.500619Z",
     "shell.execute_reply.started": "2022-04-07T08:16:13.084154Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_axes([0, 0, 1,1], aspect=1)\n",
    "ax1.pie(df_train.liquidity_rank.value_counts(normalize=True), autopct='%1.1f%%', labels=range(0,12))\n",
    "ax2 = fig.add_axes([1, 0, 1,1], aspect=1)\n",
    "ax2.pie(df_test.liquidity_rank.value_counts(normalize=True), autopct='%1.1f%%', labels=range(0,12))\n",
    "ax1.set_title('Liquidity rank frequency in train/test dataset')\n",
    "ax2.set_title('Liquidity rank frequency in validation dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c7b6f9b8-981d-43ca-98b1-34d2edc7b830",
    "_uuid": "a635260a-1dcc-4a30-acf3-0acc5ba72de6"
   },
   "source": [
    "We see that there is no substantial relative difference between the distribution of products with high liquidity rank in train/test dataset and in validation dataset. So there asre no outliers when ir comes to liquidity ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1e5e7125-d216-433e-970d-5ad28f2175b1",
    "_uuid": "389f2888-d3d0-40ba-a0b4-3ee02b9802ec",
    "execution": {
     "iopub.execute_input": "2022-04-07T08:17:19.5224Z",
     "iopub.status.busy": "2022-04-07T08:17:19.522149Z",
     "iopub.status.idle": "2022-04-07T08:17:22.351241Z",
     "shell.execute_reply": "2022-04-07T08:17:22.350558Z",
     "shell.execute_reply.started": "2022-04-07T08:17:19.52237Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.subplot(1,2,1)\n",
    "sns.distplot(df_full['spread']).set(title='Distribution of spread')\n",
    "plt.subplot(1,2,2)\n",
    "sns.boxplot(df_full['spread']).set(title='Box-plot of spread')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter on the first product ID and then on liquidity rank\n",
    "\n",
    "filter_pid = filter_data(df_full, 'product_id', 0)\n",
    "filter_pid_liq = filter_data(filter_pid, 'liquidity_rank',0)\n",
    "\n",
    "sns.boxplot(x='liquidity_rank', y='spread', data=filter_pid)\n",
    "plt.title(\"Distribution of the first product_id per liquidity rank\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e28d991a-0896-4ec1-97eb-698c86d3190e",
    "_uuid": "5c41dc9c-b963-4736-b9e1-659db7f42529"
   },
   "source": [
    "<a id='1.4'></a>\n",
    "## 1.4) Correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c427c30c-4f8c-4602-951e-a17e84a1a9f9",
    "_uuid": "db839ce3-586b-4955-afb0-acaf569daa28"
   },
   "source": [
    "Correlation within features may increase the bias in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "93c0386b-76e5-4fd8-96a0-9d01aed1cae5",
    "_uuid": "fce14c5b-c337-4467-a378-8b3149964077",
    "execution": {
     "iopub.execute_input": "2022-04-07T08:29:58.878594Z",
     "iopub.status.busy": "2022-04-07T08:29:58.878342Z",
     "iopub.status.idle": "2022-04-07T08:29:59.301233Z",
     "shell.execute_reply": "2022-04-07T08:29:59.300594Z",
     "shell.execute_reply.started": "2022-04-07T08:29:58.87856Z"
    },
    "id": "MlbWQppi6ORu",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "8e7182d6-4b01-4acf-efae-f5939d75e317"
   },
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "rs = np.random.RandomState(0)\n",
    "df = pd.DataFrame(rs.rand(10, 10))\n",
    "corr = df_train.corr()\n",
    "corr\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "642b0a00-016a-465f-8d26-af8875eceff1",
    "_uuid": "7eaabeff-8cd1-4e79-8ba7-7691b69559bd"
   },
   "source": [
    "Correlation with the label may help us build a better model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f60c6046-06a7-47e3-92a8-8cc4fb2ac2b1",
    "_uuid": "e1b34c5c-6b16-496a-b797-0a4f1a6b69cb",
    "execution": {
     "iopub.execute_input": "2022-04-07T08:30:07.105217Z",
     "iopub.status.busy": "2022-04-07T08:30:07.104648Z",
     "iopub.status.idle": "2022-04-07T08:30:07.562998Z",
     "shell.execute_reply": "2022-04-07T08:30:07.562139Z",
     "shell.execute_reply.started": "2022-04-07T08:30:07.105178Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "corr_s=df_full.corr().iloc[:-2,-1].to_frame()\n",
    "rs = np.random.RandomState(0)\n",
    "df = pd.DataFrame(rs.rand(10, 10))\n",
    "corr_s\n",
    "corr_s.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "42644d98-a906-4415-bd5e-883b2158ee93",
    "_uuid": "493293e2-cf96-442c-83c9-3b423c826737",
    "id": "4ERARJ986ORv"
   },
   "source": [
    "No significant correlation spotted. No need to go through other methods like Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7f31128a-2bbb-4005-96e1-bc4a1ced4a9b",
    "_uuid": "c8c9e886-3f6f-40b2-a697-e3e96f1bff3b",
    "id": "MCb-dzVn6ORv"
   },
   "source": [
    "Construct other variables to avoid correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1d819434-8aa0-45ef-8e9a-7b4524469be8",
    "_uuid": "e54533db-eb2f-4871-9e8e-8f84670b9c90",
    "id": "CnelIFuG6OR3"
   },
   "source": [
    "<a id='2'></a>\n",
    "# Step 2 - Features engineering and selection\n",
    "\n",
    "According to the conclusions you get from step 1 exploration, process your data (features engineering, features selection, dimensionality reduction...). Feel free to create several different datasets if you want to explore different kind of dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We reported the **one_hot_encoder** function but we finally we did not use it. This is because the ML algorithms we have chosen do not need this step: as both XGB and RF are basically built like a decision tree, then the 0-1 transformation made by the encoder is not needed anymore, since the tree will split in any case. Rather, this transformation is required when dealing with linear methods, such LASSO. In fact, categorical variables must be one-hot-encoded: their values do not mean anything in particular (ie do not have a numerical, or \"magnitude\", sense). Take for example liquidity rank: if we had to use only linear regression methods, we should have use OHE to create as many dummy variable as ranks, to make the regression correctly works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0b1a4532-bfe1-4b98-b709-dc69476eff65",
    "_uuid": "0a8e9d04-c451-44bf-bd4b-417922a6f0e7",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encoder(df, col_to_encode, col_is_int=True):\n",
    "    \"\"\"\n",
    "    Performs One Hot Encoding with sklearn.\n",
    "    df : pandas dataframe containing column to encode\n",
    "    col_to_encode : (str) column name\n",
    "    col_is_int : (bool) whether or not column is integer.\n",
    "    \"\"\"\n",
    "    # Create the mapping\n",
    "    if col_is_int: \n",
    "        integer_encoded = df[col_to_encode].values.reshape(-1, 1)\n",
    "    else:\n",
    "        values = df[col_to_encode].tolist()\n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_encoded = label_encoder.fit_transform(values)\n",
    "    \n",
    "    # One hot encoder\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    df_temp = pd.DataFrame(ohe.fit_transform(integer_encoded), index=df.index)\n",
    "    df_temp.columns = [\"%s_%s\" % (col_to_encode, col) for col in df_temp.columns]\n",
    "    df_results = pd.concat([df, df_temp], axis=1)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eefb2f42-e940-4718-a3f0-ca1e1dc61e46",
    "_uuid": "dc6b6fa6-80ae-4765-b895-841aa75b788c",
    "id": "5C_7417p6OR3"
   },
   "source": [
    "<a id='2.1'></a>\n",
    "## 2.1) Compute new features\n",
    "Some variables may not be very much significant themselves. Rather, they can explain more if combined with other ones. This can be the example of *dt_expiry* and *dt_close*: while the two categorical variable do not tell us very much about the time horizon if taken separately, we can construct a **maturity** measure that can explain more than the single variables taken singularly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "6d2ad138-a783-45ea-a799-feeb4e8318de",
    "_uuid": "ea0902a1-3fd2-4d44-90ed-323932e1c396",
    "execution": {
     "iopub.execute_input": "2022-04-09T08:19:38.508624Z",
     "iopub.status.busy": "2022-04-09T08:19:38.507628Z",
     "iopub.status.idle": "2022-04-09T08:19:38.520883Z",
     "shell.execute_reply": "2022-04-09T08:19:38.519983Z",
     "shell.execute_reply.started": "2022-04-09T08:19:38.508578Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def features_eng(df):\n",
    "    global df_full\n",
    "    def maturity(df):\n",
    "      return (df['dt_expiry'] - df['dt_close'])\n",
    "    def intraday_ret(df):\n",
    "      return (df.close - df.open)/df.open\n",
    "    def price_spread(df):\n",
    "      return (df.high - df.low)\n",
    "\n",
    "    df['mat']=maturity(df)\n",
    "    df['in_ret']=intraday_ret(df)\n",
    "    df['price_sp']=price_spread(df)\n",
    "\n",
    "    mean_target_per_product = df_full.groupby([\"product_id\",\"liquidity_rank\"])[\"spread\"].mean()\n",
    "    mean_target_per_product.name = \"mean_target_per_product\"\n",
    "    df = df.merge(mean_target_per_product, how=\"left\", right_index=True, left_on=[\"product_id\", \"liquidity_rank\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "b5d145e3-3c7f-42c4-9d9c-552f172bbda2",
    "_uuid": "b7f62b23-f3f0-4523-8bcb-bc7a30b86365",
    "execution": {
     "iopub.execute_input": "2022-04-09T08:19:40.420217Z",
     "iopub.status.busy": "2022-04-09T08:19:40.419687Z",
     "iopub.status.idle": "2022-04-09T08:19:40.663970Z",
     "shell.execute_reply": "2022-04-09T08:19:40.663219Z",
     "shell.execute_reply.started": "2022-04-09T08:19:40.420180Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_train=features_eng(df_train)\n",
    "df_test=features_eng(df_test)\n",
    "df_full = pd.concat([df_train,y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c314aff4-1292-4385-bd24-46f6c46df840",
    "_uuid": "bd0eef52-80a5-4fcd-98e4-28fdab35a53a",
    "execution": {
     "iopub.execute_input": "2022-04-07T08:30:20.336827Z",
     "iopub.status.busy": "2022-04-07T08:30:20.336311Z",
     "iopub.status.idle": "2022-04-07T08:30:20.858161Z",
     "shell.execute_reply": "2022-04-07T08:30:20.857518Z",
     "shell.execute_reply.started": "2022-04-07T08:30:20.336791Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "rs = np.random.RandomState(0)\n",
    "df = pd.DataFrame(rs.rand(10, 10))\n",
    "corr2 = df_train.corr()\n",
    "corr2\n",
    "corr2.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A powerful way to visualize relationships between data is to use a **pairplot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_full[['mat', 'in_ret', 'price_sp', 'liquidity_rank','spread']], diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We deduce the absence of any easily-spottable (linear) link between most of the features and the label. The only clear spottable link is the one maturity-liquidity_rank, which has been already identified before. So, we may want to search deeply for new features and use methods to reduce dimensionality of the dataset. We then implement PCA analysis to try reduce dimensionality of the dataset and to find some uncorrelated features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e6f22ec7-82fa-4c40-9b6f-31b0f164634a",
    "_uuid": "b0d9da39-f309-4142-b5ab-aa452f93f8a7"
   },
   "source": [
    "<a id='2.2'></a>\n",
    "### 2.2) PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_x=df_full.iloc[:,:-1]\n",
    "df_pca_y=df_full.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_test(df_pca_x.shape, (len(df_full),17), \"wrong size of df_pca_x. Current size is \"+str(df_pca_x.shape))\n",
    "check_test(df_pca_y.shape, (len(df_full),), \"wrong size of df_pca_y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "x_train, x_val, y_train, y_val = train_test_split(df_pca_x, df_pca_y, test_size = 0.2, random_state = 203)\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_val = sc.transform(x_val)\n",
    "pca = PCA()\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_val = pca.transform(x_val)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "res_PCA = pd.DataFrame({'Expl. variance' : np.round(explained_variance,2)}, index = ['PCA' + str(i) for i in range(17)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_PCA_c=np.cumsum(res_PCA)\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "\n",
    "ax1 = fig.add_axes([0, 0, 1,1])\n",
    "ax1.plot(res_PCA)\n",
    "ax2 = fig.add_axes([1, 0, 1,1])\n",
    "ax2.plot(res_PCA_c)\n",
    "ax1.set_title('PCA - Explained Variance')\n",
    "ax2.set_title('PCA - Explained Cumulative Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having examined the plots of PCA results, we decide not to proceed with it as the first components only have a scarce explanatory power. Also, we will use some machine learning algorithms that are not particularly penalised by the presence of correlation between features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "78e1eb5a-c511-4e78-b097-8d598d554f21",
    "_uuid": "caf88517-6a0f-43bd-bc42-2278f7a508cb",
    "id": "45lsdeEU6OR4"
   },
   "source": [
    "<a id='2.3'></a>\n",
    "### 2.3) Features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "23d03d4d-05e9-4b7c-bf75-c01ec574299d",
    "_uuid": "499b749e-0017-42e8-86ef-f067128eeb52",
    "id": "ueyvwtyGQVDw",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "dc4968ec-a04d-456b-c88e-1497a9e64438"
   },
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having examined the correlation matrix, we deduce that most of the variable are not significantly correlated with others, so this means that each one can significantly contribute to explain the label. We will utilize some machine learning algoriths that intrinsically detect the relative importance of the features (ie, by assigning a relative weight). This makes the task of features selection easier.\n",
    "\n",
    "Though, we include the variables that we created, such as: *mat*, *in_ret*, and *price_sp* because they can apport an increased explanatory power.\n",
    "\n",
    "We also cut off some prices since they are (as expected) very much correlated: *high*, *low*, *open*, and we only keep *close*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "aad4a4d2-2f37-4bac-bd38-f38f9b526716",
    "_uuid": "b5370e13-3fab-4057-be2d-308ec62cdac9",
    "execution": {
     "iopub.execute_input": "2022-04-09T08:19:46.214894Z",
     "iopub.status.busy": "2022-04-09T08:19:46.214326Z",
     "iopub.status.idle": "2022-04-09T08:19:46.219223Z",
     "shell.execute_reply": "2022-04-09T08:19:46.218398Z",
     "shell.execute_reply.started": "2022-04-09T08:19:46.214852Z"
    },
    "id": "NvYcpnRe6OR5",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "features = [ 'dt_close', 'product_id', 'liquidity_rank', 'normal_trading_day', 'close', 'open_interest', 'volume','tick_size', 'fixed', 'mean_target_per_product', 'mat','in_ret', 'price_sp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "b1ab0b41-01f9-4651-922a-1c5737f95b38",
    "_uuid": "409d6685-0274-4fa9-a650-9b848756f4f3",
    "execution": {
     "iopub.execute_input": "2022-04-09T08:19:46.566657Z",
     "iopub.status.busy": "2022-04-09T08:19:46.566330Z",
     "iopub.status.idle": "2022-04-09T08:19:46.571804Z",
     "shell.execute_reply": "2022-04-09T08:19:46.570860Z",
     "shell.execute_reply.started": "2022-04-09T08:19:46.566626Z"
    },
    "id": "fJGMvmP66OR5",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "b413de07-a6c8-4482-9ac3-885c24499be5"
   },
   "outputs": [],
   "source": [
    "check_test(\"ID\" not in features, True, \"error : column ID still in the features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "706df894-403e-4298-ac9f-2bb236dee666",
    "_uuid": "21b609ed-e077-4f9a-8b45-f1b240b65619",
    "id": "kUsjzT3K6OR8"
   },
   "source": [
    "<a id='3'></a>\n",
    "# Step 3 - Machine learning algorithms experiments\n",
    "\n",
    "This section includes:\n",
    "- validation procedure\n",
    "- tuned candidate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4a4e0d96-5e15-4d93-876b-0c62d3f168eb",
    "_uuid": "ef428fbc-6984-4c20-a7bf-3ed700e441a3",
    "id": "M6EmlY1I6OR8"
   },
   "source": [
    "<a id='3.1'></a>\n",
    "## 3.1) Validation procedure\n",
    "\n",
    "In this section, we will split our dataset into a train set and a validation set. Asn an example, we will use the simple **train_test_split** approach. However, you are free to choose the validation procedure. Justify your choices.\n",
    "\n",
    "- Using the train-test split approach, split your train dataframe. Assign the resulting dataframes to variables **x_train**, **x_val**, **y_train** and **y_val**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "125c64e1-2062-4cec-ab6c-e16eec592ce0",
    "_uuid": "797fbe81-6d6f-4653-bd74-f685ba51209a",
    "execution": {
     "iopub.execute_input": "2022-04-09T08:19:49.388154Z",
     "iopub.status.busy": "2022-04-09T08:19:49.387423Z",
     "iopub.status.idle": "2022-04-09T08:19:49.582591Z",
     "shell.execute_reply": "2022-04-09T08:19:49.581797Z",
     "shell.execute_reply.started": "2022-04-09T08:19:49.388114Z"
    },
    "id": "5Du824sw6OR9",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(df_train[features], y_train, random_state=203, test_size=0.2)\n",
    "x_test = df_test[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try to predict missing bid-ask spread values, we will use the following machine learning algorithms:\n",
    "- random forest\n",
    "- XGBoost\n",
    "- LASSO\n",
    "- neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.1.1'></a>\n",
    "### 3.1.1) Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "04fe8785-1bbe-45ff-995a-45fced822908",
    "_uuid": "b48cf5f0-71d6-4b3f-8b88-6b68988fdcfd"
   },
   "source": [
    "We start by running a **random forest** on all the dataset to see its explanatory power. We then proceed to estimate the optimal hyperparameters.\n",
    "\n",
    "We prefer to run many one-by-one iterations, and not a single GridSearch (or similar methods), so as to visualise the learning curves (but also because they are very dispendious in terms of computational power). The learning curves will be the decision tool to assess the (pseudo-)optimal hyperparameters, under the constraint of overfitting minimisation.\n",
    "\n",
    "We will proceed to the tuning of the following hyperparameters:\n",
    "- max_depth\n",
    "- min_sample_split\n",
    "- min_sample_leaf\n",
    "- n_estimators\n",
    "\n",
    "and we will subsequently compute use this algorithms to compute featues importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='max_depth'></a>\n",
    "#### 3.1.1.1) Random forest: max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "49ddf64a-1227-40a0-9cb8-1fcbafbc0187",
    "_uuid": "f2a39b1b-f3ee-4b1b-8a5f-270f3ab68035",
    "id": "9k0rKBgCtW1m",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "60017a1c-18db-4e55-bd1d-829ca5378293"
   },
   "outputs": [],
   "source": [
    "# RF parametrization : max_depth\n",
    "depths = range(13, 20)\n",
    "all_rmsle_train = []\n",
    "all_rmsle_val = []\n",
    "experiments  = []\n",
    "all_rmsle_train_forest = []\n",
    "all_rmsle_val_forest = []\n",
    "\n",
    "\n",
    "for depth in depths: \n",
    "\n",
    "    # Random Forest\n",
    "    clf_forest = RandomForestRegressor(n_estimators=100, max_depth=depth, n_jobs=-1)\n",
    "    model_forest = clf_forest.fit(x_train, y_train.values.ravel())\n",
    "    \n",
    "    for model, all_rmsle_train, all_rmsle_val, typ in zip([model_forest],[all_rmsle_train_forest],[all_rmsle_val_forest],[\"forest\"]):\n",
    "        # Make prediction on train, validation and test set.\n",
    "        pred_train = pd.Series(model.predict(x_train), index=y_train.index)\n",
    "        pred_val = pd.Series(model.predict(x_val), index=y_val.index)\n",
    "\n",
    "        # Compute MSLE evaluation metrics\n",
    "        rmsle_train = compute_rmsle(y_train, pred_train)\n",
    "        rmsle_val = compute_rmsle(y_val, pred_val)\n",
    "        all_rmsle_train.append(rmsle_train)\n",
    "        all_rmsle_val.append(rmsle_val)\n",
    "        print(\"depth = %s | RMSLE train = %.4f | RMSLE val = %.4f (%s)\" % (depth, rmsle_train, rmsle_val, typ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c2257654-8c24-4855-94de-60e3e88c735e",
    "_uuid": "1765a7a0-e38c-4512-99ad-585594b462c4"
   },
   "source": [
    "So we choose max_depth = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4eee828a-f36f-4cf4-824d-d6e85ef24ac4",
    "_uuid": "bfc98723-3cfd-4e2d-8164-5097551b017b",
    "id": "0Hi3ig8j3vS8",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "a017545a-c620-4a05-aab6-f8b4c32bc7bf"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12, 5])\n",
    "plt.plot(depths, all_rmsle_train, color=\"y\", label=\"Random Forest - train\")\n",
    "plt.plot(depths, all_rmsle_val, color=\"r\", label=\"Random Forest - validation\")                                                  \n",
    "plt.xlabel(\"tree depth\")\n",
    "plt.ylabel(\"RMSLE\")\n",
    "plt.title(\"Learning curves - max_depth\")\n",
    "plt.legend(loc=\"lower left\", prop={'size': 8})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='min_samples_split'></a>\n",
    "#### 3.1.1.2) Random forest: min_samples_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eb2fbe88-69df-45ea-94ae-1569a75b313f",
    "_kg_hide-output": true,
    "_uuid": "c9673bf4-3e76-4d2a-8896-d50544267dd2",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# RF parametrization : min_samples_split\n",
    "depth = 19\n",
    "min_samples = range(2,8)\n",
    "all_rmsle_train = []\n",
    "all_rmsle_val = []\n",
    "experiments  = []\n",
    "all_rmsle_train_forest = []\n",
    "all_rmsle_val_forest = []\n",
    "\n",
    "\n",
    "for min_s in min_samples: \n",
    "\n",
    "    # Random Forest\n",
    "    clf_forest = RandomForestRegressor(n_estimators=100, max_depth=depth, n_jobs=-1, min_samples_split=min_s)\n",
    "    model_forest = clf_forest.fit(x_train, y_train.values.ravel())\n",
    "    \n",
    "    for model, all_rmsle_train, all_rmsle_val, typ in zip([model_forest],[all_rmsle_train_forest],[all_rmsle_val_forest],[\"forest\"]):\n",
    "        # Make prediction on train, validation and test set.\n",
    "        pred_train = pd.Series(model.predict(x_train), index=y_train.index)\n",
    "        pred_val = pd.Series(model.predict(x_val), index=y_val.index)\n",
    "\n",
    "        # Compute MSLE evaluation metrics\n",
    "        rmsle_train = compute_rmsle(y_train, pred_train)\n",
    "        rmsle_val = compute_rmsle(y_val, pred_val)\n",
    "        all_rmsle_train.append(rmsle_train)\n",
    "        all_rmsle_val.append(rmsle_val)\n",
    "        print(\"min_samples_split = %s | RMSLE train = %.4f | RMSLE val = %.4f (%s)\" % (min_s, rmsle_train, rmsle_val, typ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "464560eb-5835-47b0-af7d-3278b3865137",
    "_uuid": "b7dcc6bc-8e2a-4f8d-bbcf-45f2a47addd6"
   },
   "source": [
    "No need to parametrize min_samples_split : no decrease in rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12, 5])\n",
    "plt.plot(min_samples, all_rmsle_train, color=\"y\", label=\"Random Forest - train\")\n",
    "plt.plot(min_samples, all_rmsle_val, color=\"r\", label=\"Random Forest - validation\")                                                  \n",
    "plt.xlabel(\"min_samples_split\")\n",
    "plt.ylabel(\"RMSLE\")\n",
    "plt.title(\"Learning curves - min_samples_split\")\n",
    "plt.legend(loc=\"lower left\", prop={'size': 8})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='min_samples_leaf'></a>\n",
    "#### 3.1.1.3) Random forest: min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "aeb6e9d8-109e-4971-bc42-95c6ca597ff6",
    "_uuid": "90e19398-17b4-41b7-8d72-061b1d15c9b3",
    "id": "FI5tOfTi8L0z",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "9329b838-2d4c-4360-c977-99fccafb538f"
   },
   "outputs": [],
   "source": [
    "# RF parametrization : min_samples_leaf\n",
    "depth = 19\n",
    "min_samples = range(2,8)\n",
    "all_rmsle_train = []\n",
    "all_rmsle_val = []\n",
    "experiments  = []\n",
    "all_rmsle_train_forest = []\n",
    "all_rmsle_val_forest = []\n",
    "\n",
    "\n",
    "for min_s in min_samples: \n",
    "\n",
    "    # Random Forest\n",
    "    clf_forest = RandomForestRegressor(n_estimators=100, max_depth=depth, n_jobs=-1, min_samples_leaf=min_s)\n",
    "    model_forest = clf_forest.fit(x_train, y_train.values.ravel())\n",
    "    \n",
    "    for model, all_rmsle_train, all_rmsle_val, typ in zip([model_forest],[all_rmsle_train_forest],[all_rmsle_val_forest],[\"forest\"]):\n",
    "        # Make prediction on train, validation and test set.\n",
    "        pred_train = pd.Series(model.predict(x_train), index=y_train.index)\n",
    "        pred_val = pd.Series(model.predict(x_val), index=y_val.index)\n",
    "\n",
    "        # Compute MSLE evaluation metrics\n",
    "        rmsle_train = compute_rmsle(y_train, pred_train)\n",
    "        rmsle_val = compute_rmsle(y_val, pred_val)\n",
    "        all_rmsle_train.append(rmsle_train)\n",
    "        all_rmsle_val.append(rmsle_val)\n",
    "        print(\"min_samples_leaf = %s | RMSLE train = %.4f | RMSLE val = %.4f (%s)\" % (min_s, rmsle_train, rmsle_val, typ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8615f692-477b-4e40-9bc9-1b22d32cbdcc",
    "_uuid": "616563f0-738f-48bf-9050-b0ed75a0e38c"
   },
   "source": [
    "\n",
    "No need to parametrize min_samples_split : no decrease in rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12, 5])\n",
    "plt.plot(min_samples, all_rmsle_train, color=\"y\", label=\"Random Forest - train\")\n",
    "plt.plot(min_samples, all_rmsle_val, color=\"r\", label=\"Random Forest - validation\")                                                  \n",
    "plt.xlabel(\"min_samples_leaf\")\n",
    "plt.ylabel(\"RMSLE\")\n",
    "plt.title(\"Learning curves - min_samples_leaf\")\n",
    "plt.legend(loc=\"lower left\", prop={'size': 8})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='n_estimators'></a>\n",
    "#### 3.1.1.4) Random forest: n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "37376eb1-dca0-41c4-938c-8373aaf45048",
    "_uuid": "4ce0dd02-d93b-4e94-8f08-911a31c1336a",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# RF parametrization : n_estimators\n",
    "depth = 19\n",
    "estims=[100, 400, 600, 800, 1000]\n",
    "all_rmsle_train = []\n",
    "all_rmsle_val = []\n",
    "experiments  = []\n",
    "all_rmsle_train_forest = []\n",
    "all_rmsle_val_forest = []\n",
    "\n",
    "\n",
    "for estim in estims: \n",
    "\n",
    "    # Random Forest\n",
    "    clf_forest = RandomForestRegressor(n_estimators=estim, max_depth=depth, n_jobs=-1)\n",
    "    model_forest = clf_forest.fit(x_train, y_train.values.ravel())\n",
    "    \n",
    "    for model, all_rmsle_train, all_rmsle_val, typ in zip([model_forest],[all_rmsle_train_forest],[all_rmsle_val_forest],[\"forest\"]):\n",
    "        # Make prediction on train, validation and test set.\n",
    "        pred_train = pd.Series(model.predict(x_train), index=y_train.index)\n",
    "        pred_val = pd.Series(model.predict(x_val), index=y_val.index)\n",
    "\n",
    "        # Compute MSLE evaluation metrics\n",
    "        rmsle_train = compute_rmsle(y_train, pred_train)\n",
    "        rmsle_val = compute_rmsle(y_val, pred_val)\n",
    "        all_rmsle_train.append(rmsle_train)\n",
    "        all_rmsle_val.append(rmsle_val)\n",
    "        print(\"n_estimators = %s | RMSLE train = %.4f | RMSLE val = %.4f (%s)\" % (estim, rmsle_train, rmsle_val, typ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9dc81b41-1bbf-4d68-92a2-f456cb1bab91",
    "_uuid": "06f0c006-2b80-4016-9e7a-7e9370d91102"
   },
   "source": [
    "This is a tricky parameter to estimate since it can easily lead to overfitting. Its parametrization depends thus on other parameters that can conterbalance its effect. For example, we could use a high n_estimators with a correspondent high shrinkage factor (like alpha or lambda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12, 5])\n",
    "plt.plot(estims, all_rmsle_train, color=\"y\", label=\"Random Forest - train\")\n",
    "plt.plot(estims, all_rmsle_val, color=\"r\", label=\"Random Forest - validation\")                                                  \n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"RMSLE\")\n",
    "plt.title(\"Learning curves - n_estimators\")\n",
    "plt.legend(loc=\"lower left\", prop={'size': 8})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **conclusions** of this tuning exercise are that the baseline model has a better predictive power on the test dataset than the tuned one. This means that the algorithm can \"balance\" itself and find its way in estimating correctly the missing bid-ask spread values. In any case, the tuning exercise is required even if we end up choosing no specific hyperparameter: otherwise, we would not have known this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Features'></a>\n",
    "#### 3.1.1.5) Features importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a random forest :\n",
    "clf = RandomForestRegressor(n_estimators=200, max_depth=12, n_jobs=-1)\n",
    "forest = clf.fit(x_train, y_train)\n",
    "\n",
    "# Compute features importances\n",
    "importances = forest.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot top 50 features\n",
    "print(\"plot top 50 features importance :\")\n",
    "plt.figure(figsize=[15,6])\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(0, 16), importances[indices][:16].tolist(), color=\"r\", align=\"center\")\n",
    "plt.xticks(range(0, 16), x_train.columns[indices[:16]], rotation=90)\n",
    "plt.xlim([-1, 17])\n",
    "plt.show()\n",
    "\n",
    "# Display full features ranking : \n",
    "print(\"Feature ranking:\")\n",
    "for f in range(x_train.shape[1]):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, x_train.columns[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "93e9dbb5-6a17-41b1-8a01-3ae6cf030e35",
    "_uuid": "20c7ac72-848f-486f-bd74-9fcf6eab2f49",
    "id": "OUAHPTumnOqP"
   },
   "source": [
    "<a id='3.1.2'></a>\n",
    "### 3.1.2) XGBoost\n",
    "\n",
    "To leverage the predictive power of XGBoost regressor, we need to tune its hyperparameters. This is mainly done to optimize the tradeoff between in-sample fit and out-of-sample predictive power. We will tune the following hyperparameters:\n",
    "- eta\n",
    "- subsample size\n",
    "- alpha \n",
    "- min_child_weights\n",
    "\n",
    "All those parameters, if well tuned, are believed maximize predictive power while minimizing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2.1) XGBoost: eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "462316c4-1d62-4035-8f10-d6f4907b4ebc",
    "_uuid": "9ee8e0f2-3273-4f9d-8f5d-bd23c5fcf0f7",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# XGB parametrization : eta\n",
    "all_rmsle_train = []\n",
    "all_rmsle_val = []\n",
    "experiments  = []\n",
    "all_rmsle_train_xgb = []\n",
    "all_rmsle_val_xgb = []\n",
    "\n",
    "etas=[0.1, 0.08, 0.06]\n",
    "\n",
    "\n",
    "for etaa in etas: \n",
    "\n",
    "    # xgb\n",
    "    model_xgb = XGBRegressor(max_depth=19, eta=etaa)\n",
    "    model_xgb.fit(x_train, y_train)\n",
    "    \n",
    "    for model, all_rmsle_train, all_rmsle_val, typ in zip([model_xgb],[all_rmsle_train_xgb],[all_rmsle_val_xgb],[\"xgboost\"]):\n",
    "        # Make prediction on train, validation and test set.\n",
    "        pred_train = pd.Series(model_xgb.predict(x_train), index=y_train.index)\n",
    "        pred_val = pd.Series(model_xgb.predict(x_val), index=y_val.index)\n",
    "\n",
    "        # Compute MSLE evaluation metrics\n",
    "        rmsle_train = compute_rmsle(y_train, pred_train)\n",
    "        rmsle_val = compute_rmsle(y_val, pred_val)\n",
    "        all_rmsle_train.append(rmsle_train)\n",
    "        all_rmsle_val.append(rmsle_val)\n",
    "        print(\"eta = %s | RMSLE train = %.4f | RMSLE val = %.4f (%s)\" % (etaa, rmsle_train, rmsle_val, typ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7524ddac-cb28-4174-82af-c320b6d7c42d",
    "_uuid": "b2cf4587-1c63-46b1-ae26-622a3bf747a5"
   },
   "source": [
    "eta = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12, 5])\n",
    "plt.plot(etas, all_rmsle_train, color=\"y\", label=\"XGBoost - train\")\n",
    "plt.plot(etas, all_rmsle_val, color=\"r\", label=\"XGBoost - validation\")                                                  \n",
    "plt.xlabel(\"eta\")\n",
    "plt.ylabel(\"RMSLE\")\n",
    "plt.title(\"Learning curves - eta\")\n",
    "plt.legend(loc=\"lower left\", prop={'size': 8})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2.2) XGBoost: subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "060359ea-8d44-4dc2-9bcb-3048f87810c5",
    "_uuid": "18f3f284-7206-49b7-828c-ade689224986",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# XGB parametrization : subsample\n",
    "all_rmsle_train = []\n",
    "all_rmsle_val = []\n",
    "experiments  = []\n",
    "all_rmsle_train_xgb = []\n",
    "all_rmsle_val_xgb = []\n",
    "\n",
    "subsamples=[0.7, 0.8, 0.9]\n",
    "\n",
    "\n",
    "for ss in subsamples: \n",
    "\n",
    "    # xgb\n",
    "    model_xgb = XGBRegressor(max_depth=19, eta=0.1, subsample=ss)\n",
    "    model_xgb.fit(x_train, y_train)\n",
    "    \n",
    "    for model, all_rmsle_train, all_rmsle_val, typ in zip([model_xgb],[all_rmsle_train_xgb],[all_rmsle_val_xgb],[\"xgboost\"]):\n",
    "        # Make prediction on train, validation and test set.\n",
    "        pred_train = pd.Series(model_xgb.predict(x_train), index=y_train.index)\n",
    "        pred_val = pd.Series(model_xgb.predict(x_val), index=y_val.index)\n",
    "\n",
    "        # Compute MSLE evaluation metrics\n",
    "        rmsle_train = compute_rmsle(y_train, pred_train)\n",
    "        rmsle_val = compute_rmsle(y_val, pred_val)\n",
    "        all_rmsle_train.append(rmsle_train)\n",
    "        all_rmsle_val.append(rmsle_val)\n",
    "        print(\"subsample = %s | RMSLE train = %.4f | RMSLE val = %.4f (%s)\" % (ss, rmsle_train, rmsle_val, typ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f7eca4cf-1dea-408a-808e-879ba2fd64d5",
    "_uuid": "bd408560-322b-46f6-bee6-9695204aebf7"
   },
   "source": [
    "subsample = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12, 5])\n",
    "plt.plot(subsamples, all_rmsle_train, color=\"y\", label=\"XGBoost - train\")\n",
    "plt.plot(subsamples, all_rmsle_val, color=\"r\", label=\"XGBoost - validation\")                                                  \n",
    "plt.xlabel(\"subsample\")\n",
    "plt.ylabel(\"RMSLE\")\n",
    "plt.title(\"Learning curves - subsample\")\n",
    "plt.legend(loc=\"lower left\", prop={'size': 8})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2.3) XGBoost: alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3b74f490-2d46-44c7-badf-238d22f4bbfe",
    "_uuid": "780f2021-a312-4b19-9ad4-43287d3a60ff",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# XGB parametrization : alpha\n",
    "all_rmsle_train = []\n",
    "all_rmsle_val = []\n",
    "experiments  = []\n",
    "all_rmsle_train_xgb = []\n",
    "all_rmsle_val_xgb = []\n",
    "\n",
    "alphas=[0.5, 0.6, 0.7]\n",
    "\n",
    "\n",
    "for als in alphas: \n",
    "\n",
    "    # xgb\n",
    "    model_xgb = XGBRegressor(max_depth=19, eta=0.1, subsample=0.7, alpha=als)\n",
    "    model_xgb.fit(x_train, y_train)\n",
    "    \n",
    "    for model, all_rmsle_train, all_rmsle_val, typ in zip([model_xgb],[all_rmsle_train_xgb],[all_rmsle_val_xgb],[\"xgboost\"]):\n",
    "        # Make prediction on train, validation and test set.\n",
    "        pred_train = pd.Series(model_xgb.predict(x_train), index=y_train.index)\n",
    "        pred_val = pd.Series(model_xgb.predict(x_val), index=y_val.index)\n",
    "\n",
    "        # Compute MSLE evaluation metrics\n",
    "        rmsle_train = compute_rmsle(y_train, pred_train)\n",
    "        rmsle_val = compute_rmsle(y_val, pred_val)\n",
    "        all_rmsle_train.append(rmsle_train)\n",
    "        all_rmsle_val.append(rmsle_val)\n",
    "        print(\"alpha = %s | RMSLE train = %.4f | RMSLE val = %.4f (%s)\" % (als, rmsle_train, rmsle_val, typ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d3f66100-4886-46b2-87d4-e9cc936e246a",
    "_uuid": "83af0233-c963-4cdc-bcc4-6f2f29c9a044"
   },
   "source": [
    "alpha = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12, 5])\n",
    "plt.plot(alphas, all_rmsle_train, color=\"y\", label=\"XGBoost - train\")\n",
    "plt.plot(alphas, all_rmsle_val, color=\"r\", label=\"XGBoost - validation\")                                                  \n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"RMSLE\")\n",
    "plt.title(\"Learning curves - alpha\")\n",
    "plt.legend(loc=\"lower left\", prop={'size': 8})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2.3) XGBoost: min_child_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e2c44da2-1779-4ee3-83df-cf13ac55005c",
    "_uuid": "b35e38df-026a-4518-9be3-9296ffc3845e",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# XGB parametrization : min_child_weights\n",
    "all_rmsle_train = []\n",
    "all_rmsle_val = []\n",
    "experiments  = []\n",
    "all_rmsle_train_xgb = []\n",
    "all_rmsle_val_xgb = []\n",
    "\n",
    "min_child_weights=[0.5, 1.5, 2]\n",
    "\n",
    "\n",
    "for mcw in min_child_weights: \n",
    "\n",
    "    # xgb\n",
    "    model_xgb = XGBRegressor(max_depth=19, eta=0.1, subsample=0.7, alpha=0.7, min_child_weight=mcw)\n",
    "    model_xgb.fit(x_train, y_train)\n",
    "    \n",
    "    for model, all_rmsle_train, all_rmsle_val, typ in zip([model_xgb],[all_rmsle_train_xgb],[all_rmsle_val_xgb],[\"xgboost\"]):\n",
    "        # Make prediction on train, validation and test set.\n",
    "        pred_train = pd.Series(model_xgb.predict(x_train), index=y_train.index)\n",
    "        pred_val = pd.Series(model_xgb.predict(x_val), index=y_val.index)\n",
    "\n",
    "        # Compute MSLE evaluation metrics\n",
    "        rmsle_train = compute_rmsle(y_train, pred_train)\n",
    "        rmsle_val = compute_rmsle(y_val, pred_val)\n",
    "        all_rmsle_train.append(rmsle_train)\n",
    "        all_rmsle_val.append(rmsle_val)\n",
    "        print(\"min_child_weight = %s | RMSLE train = %.4f | RMSLE val = %.4f (%s)\" % (mcw, rmsle_train, rmsle_val, typ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12, 5])\n",
    "plt.plot(min_child_weights, all_rmsle_train, color=\"y\", label=\"XGBoost - train\")\n",
    "plt.plot(min_child_weights, all_rmsle_val, color=\"r\", label=\"XGBoost - validation\")                                                  \n",
    "plt.xlabel(\"min_child_weights\")\n",
    "plt.ylabel(\"RMSLE\")\n",
    "plt.title(\"Learning curves - min_child_weights\")\n",
    "plt.legend(loc=\"lower left\", prop={'size': 8})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.1.3'></a>\n",
    "### 3.1.3) LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run a linear (LASSO) regression to see if the candidate models contribute to RMSE minimization or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d711ee1c-c54f-417b-a33e-1ef8c9a0ba4c",
    "_uuid": "eeb578d1-3572-4f1d-a50c-e21d525ac72f",
    "execution": {
     "iopub.execute_input": "2022-04-03T21:53:09.746753Z",
     "iopub.status.busy": "2022-04-03T21:53:09.745894Z",
     "iopub.status.idle": "2022-04-03T21:53:09.768491Z",
     "shell.execute_reply": "2022-04-03T21:53:09.767416Z",
     "shell.execute_reply.started": "2022-04-03T21:53:09.746708Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "alphas=[0.5,0.1,0.01, 0.001]\n",
    "\n",
    "for als in alphas: \n",
    "\n",
    "    # xgb\n",
    "    reg = Lasso(alpha=als)\n",
    "    model_lin = reg.fit(x_train,y_train)\n",
    "    \n",
    "    for model, all_rmsle_train, all_rmsle_val, typ in zip([model_lin],[all_rmsle_train_xgb],[all_rmsle_val_xgb],[\"LASSO\"]):\n",
    "        # Make prediction on train, validation and test set.\n",
    "        pred_train_lin = pd.Series(model_lin.predict(x_train), index=y_train.index)\n",
    "        pred_val_lin = pd.Series(model_lin.predict(x_val), index=y_val.index)\n",
    "\n",
    "        # Compute MSLE evaluation metrics\n",
    "        rmsle_train_lin = compute_rmsle(y_train, pred_train_lin)\n",
    "        rmsle_val_lin = compute_rmsle(y_val, pred_val_lin)\n",
    "        all_rmsle_train.append(rmsle_train_lin)\n",
    "        all_rmsle_val.append(rmsle_val_lin)\n",
    "        print(\"alpha = %s | RMSLE train = %.4f | RMSLE val = %.4f (%s)\" % (als, rmsle_train_lin, rmsle_val_lin, typ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f953f506-7ced-45a2-bd62-08f8c230fa00",
    "_uuid": "1b944442-6605-4243-a2cf-15dcc3902e02",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "reg = Lasso(alpha=0.01)\n",
    "model_lin = reg.fit(x_train,y_train)\n",
    "\n",
    "pred_train_lin = model_lin.predict(x_train)\n",
    "pred_val_lin = model_lin.predict(x_val)\n",
    "pred_test_lin = model_lin.predict(df_test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.1.4'></a>\n",
    "### 3.1.4) Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters that we should tune are:\n",
    "- number of intermediate layers to add to the network\n",
    "- inputs to consider\n",
    "- loss function\n",
    "- validation method\n",
    "- number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T08:23:57.158395Z",
     "iopub.status.busy": "2022-04-09T08:23:57.157705Z",
     "iopub.status.idle": "2022-04-09T08:23:57.163287Z",
     "shell.execute_reply": "2022-04-09T08:23:57.162618Z",
     "shell.execute_reply.started": "2022-04-09T08:23:57.158355Z"
    }
   },
   "outputs": [],
   "source": [
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=11, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T08:53:43.007667Z",
     "iopub.status.busy": "2022-04-09T08:53:43.007397Z",
     "iopub.status.idle": "2022-04-09T08:59:26.966572Z",
     "shell.execute_reply": "2022-04-09T08:59:26.965875Z",
     "shell.execute_reply.started": "2022-04-09T08:53:43.007637Z"
    }
   },
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=1, batch_size=3, verbose=2)\n",
    "kfold = KFold(n_splits=2)\n",
    "results = cross_val_score(estimator, x_train, y_train, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T09:09:01.784819Z",
     "iopub.status.busy": "2022-04-09T09:09:01.784542Z",
     "iopub.status.idle": "2022-04-09T09:09:01.792624Z",
     "shell.execute_reply": "2022-04-09T09:09:01.791754Z",
     "shell.execute_reply.started": "2022-04-09T09:09:01.784788Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the model\n",
    "estimator.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T09:19:44.707135Z",
     "iopub.status.busy": "2022-04-09T09:19:44.706592Z",
     "iopub.status.idle": "2022-04-09T09:22:57.086414Z",
     "shell.execute_reply": "2022-04-09T09:22:57.085676Z",
     "shell.execute_reply.started": "2022-04-09T09:19:44.707095Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model_nn=estimator\n",
    "model_nn.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T09:22:57.088331Z",
     "iopub.status.busy": "2022-04-09T09:22:57.088081Z",
     "iopub.status.idle": "2022-04-09T09:28:03.087242Z",
     "shell.execute_reply": "2022-04-09T09:28:03.085805Z",
     "shell.execute_reply.started": "2022-04-09T09:22:57.088295Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the predictions\n",
    "pred_train_nn = pd.Series(model_nn.predict(x_train), index=y_train.index)\n",
    "pred_val_nn = pd.Series(model_nn.predict(x_val), index=y_val.index)\n",
    "pred_test_nn = pd.Series(model_nn.predict(x_test), index=df_test.index)\n",
    "\n",
    "rmse_train_nn = mean_squared_error(pred_train_nn, y_train, squared=False)\n",
    "rmse_val_nn = mean_squared_error(pred_val_nn, y_val, squared=False)\n",
    "\n",
    "print(\"RMSE score on train dataset : %.4f\" % rmse_train_nn)\n",
    "print(\"RMSE score on validation dataset : %.4f\" % rmse_val_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE score of 5.18 denotes a scarce predictive power (even if we did not proceed to fine-tune the model, but other algorithms never deliveres such a high score!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.2'></a>\n",
    "## 3.2) Tuned candidate models\n",
    "\n",
    "After several trials, we finally end up eliminating the features we had previously created as the model without them performs better than the one with them. So we need to re-define the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescue_me()\n",
    "\n",
    "features = [ 'dt_close', 'product_id', 'liquidity_rank', 'normal_trading_day', 'close', 'open_interest', 'volume','tick_size', 'fixed', 'mean_target_per_product', 'mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(df_train[features], y_train, random_state=203, test_size=0.2)\n",
    "x_test = df_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0073f4d9-16f0-4d5c-ba77-340e27111942",
    "_uuid": "d127b11a-ff65-4ad7-b50d-d41912ed2217",
    "execution": {
     "iopub.execute_input": "2022-04-03T21:40:30.147011Z",
     "iopub.status.busy": "2022-04-03T21:40:30.146369Z",
     "iopub.status.idle": "2022-04-03T21:46:04.312672Z",
     "shell.execute_reply": "2022-04-03T21:46:04.311908Z",
     "shell.execute_reply.started": "2022-04-03T21:40:30.146973Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_xgb = XGBRegressor(n_estimators=200, max_depth=16, eta=0.05, subsample=0.7, colsample_bytree=0.8, reg_lambda=1.5)\n",
    "model_xgb.fit(x_train, y_train)\n",
    "\n",
    "pred_train = pd.Series(model_xgb.predict(x_train), index=y_train.index)\n",
    "pred_val = pd.Series(model_xgb.predict(x_val), index=y_val.index)\n",
    "pred_test = pd.Series(model_xgb.predict(x_test), index=df_test.index)\n",
    "\n",
    "rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
    "rmse_val = mean_squared_error(pred_val, y_val, squared=False)\n",
    "\n",
    "print(\"RMSE score on train dataset : %.4f\" % rmse_train)\n",
    "print(\"RMSE score on validation dataset : %.4f\" % rmse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "506b69f7-728a-48ff-a07b-3f0eb161220e",
    "_uuid": "76c6afcc-2edf-4ac5-b3a1-a6619a0e2c80",
    "execution": {
     "iopub.execute_input": "2022-04-03T21:46:04.314582Z",
     "iopub.status.busy": "2022-04-03T21:46:04.314208Z",
     "iopub.status.idle": "2022-04-03T21:53:01.12476Z",
     "shell.execute_reply": "2022-04-03T21:53:01.124028Z",
     "shell.execute_reply.started": "2022-04-03T21:46:04.314542Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "clf_forest = RandomForestRegressor()\n",
    "model_forest = clf_forest.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "pred_train_for = pd.Series(model_forest.predict(x_train), index=y_train.index)\n",
    "pred_val_for = pd.Series(model_forest.predict(x_val), index=y_val.index)\n",
    "pred_test_for = pd.Series(model_forest.predict(x_test), index=df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "804abb58-160a-43da-9989-e5e4358c598c",
    "_uuid": "9bbf1108-3f42-4ec6-9e38-636ca8b009da",
    "execution": {
     "iopub.execute_input": "2022-04-03T21:01:12.473041Z",
     "iopub.status.busy": "2022-04-03T21:01:12.472321Z",
     "iopub.status.idle": "2022-04-03T21:01:12.487963Z",
     "shell.execute_reply": "2022-04-03T21:01:12.48726Z",
     "shell.execute_reply.started": "2022-04-03T21:01:12.472993Z"
    },
    "id": "aMg5d_h0PX42",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "196182f1-ea15-48af-e97e-0ad0913e4b87"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "#xgb\n",
    "rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
    "rmse_val = mean_squared_error(pred_val, y_val, squared=False)\n",
    "\n",
    "print(\"RMSE score on train dataset : %.4f\" % rmse_train)\n",
    "print(\"RMSE score on validation dataset : %.4f\" % rmse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c211247a-21fb-402c-831b-973bdbd4b52b",
    "_uuid": "cb684bee-888c-4438-bd67-091a78c8c3fe",
    "execution": {
     "iopub.execute_input": "2022-04-02T12:11:15.048076Z",
     "iopub.status.busy": "2022-04-02T12:11:15.047818Z",
     "iopub.status.idle": "2022-04-02T12:11:15.062567Z",
     "shell.execute_reply": "2022-04-02T12:11:15.061116Z",
     "shell.execute_reply.started": "2022-04-02T12:11:15.048047Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#RF\n",
    "rmse_train_for = mean_squared_error(pred_train_for, y_train, squared=False)\n",
    "rmse_val_for = mean_squared_error(pred_val_for, y_val, squared=False)\n",
    "\n",
    "print(\"RMSE score on train dataset : %.4f\" % rmse_train_for)\n",
    "print(\"RMSE score on validation dataset : %.4f\" % rmse_val_for)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d0b426eb-ecf9-49f6-8ce3-297dab7954f6",
    "_uuid": "d9894595-7ba1-40a2-acaf-5f8eb1f00c69"
   },
   "source": [
    "The results should be the following:\n",
    "\n",
    "**XGB**\n",
    "\n",
    "XGBRegressor(n_estimators=200, max_depth=16, eta=0.05, subsample=0.7, colsample_bytree=0.8, reg_lambda=1.5)\n",
    "- RMSE score on train dataset : 0.3333\n",
    "- RMSE score on validation dataset : 0.6708\n",
    "\n",
    "**RF**\n",
    "\n",
    "RandomForestRegressor()\n",
    "- RMSE score on train dataset : 0.2588\n",
    "- RMSE score on validation dataset : 0.6903"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bf42000f-7e27-4a27-8812-b0b74cf6425f",
    "_uuid": "128d823f-6f39-43f6-84e0-6d68ccdd8e15"
   },
   "source": [
    "Evaluation of the mixed composition: XGB + RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f06bb92d-e931-48ff-8ba6-8901d1af6ead",
    "_uuid": "8436967d-83d1-4bf5-8a52-69f7d47a6024",
    "execution": {
     "iopub.execute_input": "2022-04-03T21:53:18.706819Z",
     "iopub.status.busy": "2022-04-03T21:53:18.706113Z",
     "iopub.status.idle": "2022-04-03T21:53:18.977076Z",
     "shell.execute_reply": "2022-04-03T21:53:18.976345Z",
     "shell.execute_reply.started": "2022-04-03T21:53:18.706784Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "a = np.linspace(0.1, 0.9, 90)\n",
    "\n",
    "final_rmsle_train = []\n",
    "final_rmsle_val = []\n",
    "\n",
    "for aa in a:\n",
    "    pred_train_comp=aa*pred_train+(1-aa)*pred_train_for\n",
    "    pred_val_comp=aa*pred_val+(1-aa)*pred_val_for\n",
    "\n",
    "    rmse_train_comp = mean_squared_error(pred_train_comp, y_train, squared=False)\n",
    "    rmse_val_comp = mean_squared_error(pred_val_comp, y_val, squared=False)\n",
    "\n",
    "    final_rmsle_train.append(rmse_train_comp)\n",
    "    final_rmsle_val.append(rmse_val_comp)\n",
    "\n",
    "    \n",
    "fig = plt.figure(figsize=[12, 5])\n",
    "plt.plot(a, final_rmsle_train, color=\"y\", label=\"train\")\n",
    "plt.plot(a, final_rmsle_val, color=\"r\", label=\"validation\")                                                  \n",
    "plt.xlabel(\"weight\")\n",
    "plt.ylabel(\"RMSLE\")\n",
    "plt.title(\"Learning curves - playing on weighting scheme\")\n",
    "plt.legend(loc=\"lower left\", prop={'size': 8})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8e892630-65c4-4963-b65c-6c6b9e8e6884",
    "_uuid": "ea684ff8-c020-4cb0-91ef-2ed12ab8b1b2",
    "execution": {
     "iopub.execute_input": "2022-04-02T12:14:25.968068Z",
     "iopub.status.busy": "2022-04-02T12:14:25.967802Z",
     "iopub.status.idle": "2022-04-02T12:14:25.974633Z",
     "shell.execute_reply": "2022-04-02T12:14:25.973486Z",
     "shell.execute_reply.started": "2022-04-02T12:14:25.968037Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "final_rmsle_val, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2eb0412f-9fab-43a1-9d04-a5929cd52d8e",
    "_uuid": "ec4c5eed-f560-4f97-aeb0-77b8349552a7"
   },
   "source": [
    "So we have noticed that a mix of two methods is better than one. But how to choose the weight to assigno to each model? We can try all the combinations and plot them to see where the curve of RMSLE reaches its global minimun. Then we will compute our final pred_test as this linear combination of XGBoost and Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "00cc047b-0af8-445e-945d-c5bcc5b54646",
    "_uuid": "983fec93-daf9-45d9-9c6f-a50a5860acc6"
   },
   "source": [
    "<a id='4'></a>\n",
    "# Step 4 - Final run\n",
    "\n",
    "According to previous steps results and your own interpretation, run your best machine learning algorithm and make  predictions on the test set. Then export your results in and make submission on Kaggle platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e3e11336-20ea-4eaf-a72c-3ffe24d65c7b",
    "_uuid": "b8a69ad4-ae2c-4d37-99d5-1772fee14a07"
   },
   "source": [
    "<a id='4.1'></a>\n",
    "### 4.1) Run your best model and make prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5e6ce3a7-6d93-4a81-b4b4-d7b033f96e4a",
    "_uuid": "fcca5278-5380-4193-91da-66a6f5e436f0",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "check_test(pred_test.shape, (230304, 1), \"wrong shape for pred test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "21fc0b15-f174-4b63-982e-c6ceec9d08f5",
    "_uuid": "4431a62c-2724-4c0b-8373-5593927151fc"
   },
   "source": [
    "**Evaluate best model**\n",
    "\n",
    "Our best machine learning algorithm yields the following results :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1cfe28f4-fe4f-468e-ba92-f8efc9a56514",
    "_uuid": "ba23a991-3b04-4e79-b305-bffb2b725ef4",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
    "rmse_val = mean_squared_error(pred_val, y_val, squared=False)\n",
    "\n",
    "print(\"RMSE score on train dataset : %.4f\" % rmse_train)\n",
    "print(\"RMSE score on validation dataset : %.4f\" % rmse_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "168d43b0-9fe7-4f27-8a88-6cf280ef5fa0",
    "_uuid": "d7497dbd-a4ac-4a66-8e4b-3bbfe85d84fc",
    "id": "SN9NEmSv6OSH"
   },
   "source": [
    "<a id='4.2'></a>\n",
    "### 4.2) Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that XGBoost and Random Forest are the best algorithms to predict the missing bid-ask spread values. Other Machine Learning algorithms like LASSO regression or Neural Networks are not adequate for this scope. This can be because:\n",
    "\n",
    "- **LASSO is a linear method**: even if it is built upon a shrinkage loss function, LASSO is still a linear method. This means that the labes is supposed to be a linear combination of the features. During the first part of this work, we have shown how the dataset is very skewed and present fat tails. For example, we have seen that spreads tend to have less frequency as its value comes close to 10, but then there is an abrubt peak at this same value of 10. LASSO is not able to model this highly asymmetric relationship and it is thus discarded.\n",
    "    \n",
    "- **Neural Networks may be too complex for this scope**: NN performance is highly dependent on the hardware on which they are run. They are also dependent on many hyperparameters that require a lot of tipe and computational power to be tuned. Even using the GPU accelerator on Kaggle, this task may require many hours. And there is no guaranty that the predictiv epower will be higher that the one reached with XGBoost/Random Forest. So we discard this model since we already reached satisfactory results with the combination af the abovementioned models.\n",
    "\n",
    "On the other side, we find successfully that:\n",
    "\n",
    "- **A well tuned XGB yields good results**: this is due basically to the way the regressor is built: for example, performing bagging, or choosing a random subsample of features, or adding a shrinkage coefficient (all together), contributes to a superior performance. Also, its tuning is more \"user-friendly\" since the user can practically see how the predictive power evolves with the variation of hyperparameters (through the learning curves).\n",
    "\n",
    "- **Random forest is better if paired withh XGB**: even if we have seen that, taken singularly, Rnadom forest produces weakest results than XGB, their combinied use increases the predictive power of the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e08c545d-5696-46da-95d3-28e1f56b3d14",
    "_uuid": "9ef43d05-07f4-4154-a6d4-615a05de154e",
    "id": "nTsGgaTE6OSI"
   },
   "source": [
    "**Export submission to ENS data challenge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0c5ebae0-9e62-409c-a9d8-5c7ba8e0a78f",
    "_uuid": "1de54dc3-7242-4a6f-bf34-17f5e055d7ee",
    "execution": {
     "iopub.execute_input": "2022-04-03T21:53:43.71156Z",
     "iopub.status.busy": "2022-04-03T21:53:43.71129Z",
     "iopub.status.idle": "2022-04-03T21:53:43.718311Z",
     "shell.execute_reply": "2022-04-03T21:53:43.71756Z",
     "shell.execute_reply.started": "2022-04-03T21:53:43.711531Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pred_test_final=0.7*pred_test+0.3*pred_test_for2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4624e196-c010-42a8-9e4a-5fd50bcc2e8c",
    "_uuid": "3643686f-5cc2-46f4-9257-817ef8509537",
    "execution": {
     "iopub.execute_input": "2022-04-03T21:53:49.953486Z",
     "iopub.status.busy": "2022-04-03T21:53:49.953231Z",
     "iopub.status.idle": "2022-04-03T21:53:50.689267Z",
     "shell.execute_reply": "2022-04-03T21:53:50.688412Z",
     "shell.execute_reply.started": "2022-04-03T21:53:49.953458Z"
    },
    "id": "SAzN9OD06OSI",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_submit = export_ens(df_test, pred_test_final, True, \"my_first_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "375f38cb-79f5-45b2-84b8-236feb87e166",
    "_uuid": "0e1bf060-5149-4cb8-9411-cdd1bc618a2f",
    "id": "jfGntePS6OSJ"
   },
   "source": [
    "<a id='4.3'></a>\n",
    "### 4.3) Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "We have constructed a whole machine learning pipeline that allows us to predict missing bid-ask spread quotes. The pipeline starts with <b>data exploration and visualisation</b>: here we had explored some peculiarities of our features and have tried to enhance their explanatory power by contructing new ones or improving the existing ones.\n",
    "\n",
    "We have then worked on the features to select them and to make sure that the algorithm can work well.\n",
    "\n",
    "We have subsequently tunned some algorithms that are considered to work well for this topic (random forest and xgboost). We have tuned their hyperparameters one by one to explore the shape of the learning curves. Our objective here was to optimiwe the trade-off between in-sample fit and ou-of-sample predictive power (in other words, to minimize overfitting). \n",
    "    \n",
    "We have found that their combined use can yield better results than their individual use. In other words, weighting accurately the outcomes of the two models has a superior predictive power than the two dinstinct individual outcomes.\n",
    "</div>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
